{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider\n",
    "\n",
    "- The source of the data OWM or OM\n",
    "- Which models to train rfr, xgb, knn, ridge by settting the True or False labels\n",
    "- Which models to train by giving the Deep Learning names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(data_ = f'data\\Sere Wind Farm_hourly_OWM.csv', train_ = False, uni = True, window_size = 24*4, step = 24, sanity_check = False, tensor_ = False):\n",
    "    if uni:\n",
    "        column_ = 0\n",
    "    else:\n",
    "        column_ = None\n",
    "\n",
    "    dm = WeatherDataModule(data_dir=data_, \n",
    "                        window_size=window_size, column=column_,\n",
    "                        batch_size=32, step_=step, \n",
    "                        normalize_=True, return_tensor=tensor_)\n",
    "\n",
    "    dm.prepare_data()\n",
    "    dm.setup('')\n",
    "\n",
    "    if sanity_check:\n",
    "        plt.plot(np.arange(window_size),dm.f_test[0], label='Input')\n",
    "        if step == 1:\n",
    "            plt.scatter(np.arange(window_size, window_size+step),dm.t_test[0], label='Target', s=5, c='r')\n",
    "        else:\n",
    "            plt.plot(np.arange(window_size, window_size+step),dm.t_test[0], label='Target', c='r')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return dm\n",
    "\n",
    "def train_deep_models(dm, window_size, step, source, name, folder='deep_models', verbose = 1):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model = build_model(hidden_size=[64, 32], out=step, input_shape_= window_size, type_=name)\n",
    "\n",
    "    checkpoint_path = f\"{folder}/keras_model_{name}_ws_{window_size}_{step}_{source}.h5\"\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(dm.f_train, dm.t_train, validation_data=(dm.f_valid, dm.t_valid), epochs=150, batch_size=32, verbose=verbose, callbacks=[early_stop, checkpoint])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6132, 144)\n",
      "Valid: (1752, 144)\n",
      "Test: (694, 144)\n"
     ]
    }
   ],
   "source": [
    "window_size = 24*6\n",
    "step = 38\n",
    "source = 'ERA'\n",
    "save_folder = 'models_compare'\n",
    "train_reg = True\n",
    "train_deep = True\n",
    "\n",
    "# dm = setup_data(data_=f'data\\Sere Wind Farm_hourly_{source}.csv', train_ = False, uni = True, window_size = window_size, step = step, sanity_check = False, tensor_=False)\n",
    "dm = setup_data(data_=f'ERA5_Data\\ERA5_Reanalysis.csv', train_ = False, uni = True, window_size = window_size, step = step, sanity_check = False, tensor_=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Regressor...\n",
      "Elapsed minutes: 2.1648295203844707\n",
      "\n",
      "\n",
      "\n",
      "Training XGBoost Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [16:43:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed minutes: 1.2796406428019205\n",
      "\n",
      "\n",
      "\n",
      "Training KNN Regressor...\n",
      "Elapsed minutes: 8.751948674519856e-05\n",
      "\n",
      "\n",
      "\n",
      "Training ridge Regressor...\n",
      "Elapsed minutes: 0.0005638957023620606\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [16:44:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "if train_reg:\n",
    "    train(dm= dm, folder=save_folder, train_models=True, rfr=True, xgb_=True, knn=True, ridge=True, window_size=window_size, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_size = [128, 64], out = 1, input_shape_ = 24 * 2, type_ = 'DNN'):\n",
    "    if type_ == 'DNN':\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_size[0], input_shape=(input_shape_,)))\n",
    "        model.add(Dense(hidden_size[1], activation='relu'))\n",
    "        model.add(Dense(out))\n",
    "    \n",
    "    elif type_ == 'LSTM':\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(hidden_size[0], return_sequences=True, input_shape=(input_shape_, 1)))\n",
    "        model.add(LSTM(hidden_size[1], activation='relu'))\n",
    "        # model.add(Dense(hidden_size[1], activation='relu'))\n",
    "        model.add(Dense(out))\n",
    "    \n",
    "    elif type_ == 'GRU':\n",
    "        model = Sequential()\n",
    "        model.add(GRU(hidden_size[0], input_shape=(input_shape_, 1)))\n",
    "        model.add(Dense(hidden_size[1], activation='relu'))\n",
    "        model.add(Dense(out))\n",
    "\n",
    "    elif type_ == 'CNN':\n",
    "        '''\n",
    "        The CNN is more effective when using less layers and filters\n",
    "\n",
    "        Hyperparameters will include:\n",
    "        - Number of filters or hidden size\n",
    "        - Kernel size\n",
    "        - Total layers\n",
    "        '''\n",
    "        model = tf.keras.Sequential([\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu', input_shape=(input_shape_, 1)),\n",
    "\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Dense(512, activation='relu'),\n",
    "\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(out, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    elif type_ == 'test':\n",
    "        model = tf.keras.Sequential([\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu', input_shape=(input_shape_, 1)),\n",
    "\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            LSTM(64, activation='relu'),\n",
    "\n",
    "            Flatten(),            \n",
    "\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(out, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model\n",
      "Epoch 1/150\n",
      "192/192 [==============================] - 26s 122ms/step - loss: 0.0404 - val_loss: 0.0267\n",
      "Epoch 2/150\n",
      "  1/192 [..............................] - ETA: 19s - loss: 0.0196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 18s 96ms/step - loss: 0.0268 - val_loss: 0.0266\n",
      "Epoch 3/150\n",
      "192/192 [==============================] - 20s 107ms/step - loss: 0.0261 - val_loss: 0.0266\n",
      "Epoch 4/150\n",
      "192/192 [==============================] - 19s 97ms/step - loss: 0.0253 - val_loss: 0.0244\n",
      "Epoch 5/150\n",
      "192/192 [==============================] - 18s 94ms/step - loss: 0.0247 - val_loss: 0.0236\n",
      "Epoch 6/150\n",
      "192/192 [==============================] - 18s 93ms/step - loss: 0.0242 - val_loss: 0.0235\n",
      "Epoch 7/150\n",
      "192/192 [==============================] - 17s 89ms/step - loss: 0.0238 - val_loss: 0.0231\n",
      "Epoch 8/150\n",
      "192/192 [==============================] - 17s 87ms/step - loss: 0.0234 - val_loss: 0.0220\n",
      "Epoch 9/150\n",
      "192/192 [==============================] - 19s 98ms/step - loss: 0.0230 - val_loss: 0.0225\n",
      "Epoch 10/150\n",
      "192/192 [==============================] - 18s 96ms/step - loss: 0.0229 - val_loss: 0.0225\n",
      "Epoch 11/150\n",
      "192/192 [==============================] - 18s 96ms/step - loss: 0.0226 - val_loss: 0.0219\n",
      "Epoch 12/150\n",
      "192/192 [==============================] - 18s 93ms/step - loss: 0.0225 - val_loss: 0.0222\n",
      "Epoch 13/150\n",
      "192/192 [==============================] - 19s 100ms/step - loss: 0.0224 - val_loss: 0.0218\n",
      "Epoch 14/150\n",
      "192/192 [==============================] - 18s 92ms/step - loss: 0.0222 - val_loss: 0.0222\n",
      "Epoch 15/150\n",
      "192/192 [==============================] - 17s 91ms/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 16/150\n",
      "192/192 [==============================] - 18s 94ms/step - loss: 0.0218 - val_loss: 0.0223\n"
     ]
    }
   ],
   "source": [
    "if train_deep:\n",
    "    model_names = ['LSTM'] #['DNN', 'LSTM', 'GRU', 'CNN']\n",
    "\n",
    "    for name in model_names:\n",
    "        print(f'Training {name} model')\n",
    "        train_deep_models(dm, window_size, step, source, name, folder=save_folder, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
