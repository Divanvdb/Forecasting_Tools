{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider\n",
    "\n",
    "- The source of the data OWM or OM\n",
    "- Which models to train rfr, xgb, knn, ridge by settting the True or False labels\n",
    "- Which models to train by giving the Deep Learning names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shared_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(data_ = f'data\\Sere Wind Farm_hourly_OWM.csv', train_ = False, uni = True, window_size = 24*4, step = 24, sanity_check = False, tensor_ = False, inc = 4):\n",
    "    if uni:\n",
    "        column_ = 0\n",
    "    else:\n",
    "        column_ = None\n",
    "\n",
    "    dm = WeatherDataModule(data_dir=data_, \n",
    "                        window_size=window_size, column=column_,\n",
    "                        batch_size=32, step_=step, \n",
    "                        normalize_=True, return_tensor=tensor_, inc = inc)\n",
    "\n",
    "    dm.prepare_data()\n",
    "    dm.setup('')\n",
    "\n",
    "    if sanity_check:\n",
    "        plt.plot(np.arange(window_size),dm.f_test[0], label='Input')\n",
    "        if step == 1:\n",
    "            plt.scatter(np.arange(window_size, window_size+step),dm.t_test[0], label='Target', s=5, c='r')\n",
    "        else:\n",
    "            plt.plot(np.arange(window_size, window_size+step),dm.t_test[0], label='Target', c='r')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return dm\n",
    "\n",
    "def train_deep_models(dm, window_size, step, source, name, folder='deep_models', verbose = 1):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model = build_model(hidden_size=[64, 32], out=step, input_shape_= window_size, type_=name)\n",
    "\n",
    "    checkpoint_path = f\"{folder}/keras_model_{name}_ws_{window_size}_{step}_{source}.h5\"\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(dm.f_train, dm.t_train, validation_data=(dm.f_valid, dm.t_valid), epochs=150, batch_size=32, verbose=verbose, callbacks=[early_stop, checkpoint])\n",
    "\n",
    "    return model\n",
    "\n",
    "class WeatherDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"data\\current_weather_data.csv\", index_='timestamp', \n",
    "                 column=0, batch_size=64, window_size=5, normalize_=False,\n",
    "                 date_range = None, step_ = 24, return_tensor = True, inc = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.index_ = index_\n",
    "        self.column = column\n",
    "        self.date_range = date_range\n",
    "        self.window_size = window_size\n",
    "        self.step_ = step_\n",
    "        self.return_tensor = return_tensor\n",
    "        self.inc = inc\n",
    "        self.normalize_ = normalize_\n",
    "\n",
    "    def prepare_data(self):\n",
    "        df_ = pd.read_csv(self.data_dir, index_col=self.index_, parse_dates=True)\n",
    "        if self.date_range != None:\n",
    "            df_ = df_[self.date_range]\n",
    "        \n",
    "        if self.column == None:\n",
    "            self.df = df_\n",
    "        else:\n",
    "            self.df = df_.iloc[:,self.column]\n",
    "        \n",
    "        if self.normalize_:\n",
    "            self.scaler = MinMaxScaler()\n",
    "            self.scaler.fit(self.df.values.reshape(-1, 1))\n",
    "            self.df = self.normalize(self.df)\n",
    "\n",
    "        self.windows, self.targets, self.s_t, self.s_f = self.window_step(self.df, self.step_, inc = self.inc)\n",
    "\n",
    "    def window_step(self, dataset, step_, inc ):\n",
    "        \"\"\"Transform a time series into a prediction dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset: A numpy array of time series, first dimension is the time steps\n",
    "            lookback: Size of window for prediction\n",
    "        \"\"\"\n",
    "        X, y, st, sf = [], [], [], []\n",
    "        for i in range(len(dataset)-self.window_size - step_ * inc):\n",
    "            feature = dataset[i:i+self.window_size]\n",
    "            target = dataset[i+self.window_size:i+self.window_size+step_]\n",
    "            window = dataset[i:i+self.window_size:inc]\n",
    "            stagered = dataset[i+self.window_size:i+self.window_size+step_:inc]\n",
    "            X.append(feature)\n",
    "            y.append(target)\n",
    "            st.append(stagered)\n",
    "            sf.append(window)\n",
    "\n",
    "        if self.return_tensor:\n",
    "            X_r = torch.tensor(np.array(X))\n",
    "            y_r = torch.tensor(np.array(y))\n",
    "            return X_r.float(), y_r.float()\n",
    "        else:\n",
    "            return np.array(X).squeeze(), np.array(y).squeeze(), np.array(st).squeeze(), np.array(sf).squeeze()\n",
    "    \n",
    "    def normalize(self, series):\n",
    "        if self.column == None:\n",
    "            return pd.DataFrame(self.scaler.fit_transform(series), index=series.index)\n",
    "        else:\n",
    "            return pd.DataFrame(self.scaler.fit_transform(series.values.reshape(-1, 1)), index=series.index)\n",
    "    \n",
    "    def inverse_normalze(self, series):\n",
    "        if self.column == None:\n",
    "            return pd.DataFrame(self.scaler.inverse_transform(series), index=series.index)\n",
    "        else:\n",
    "            return pd.DataFrame(self.scaler.inverse_transform(series.values.reshape(-1, 1)), index=series.index)\n",
    "    \n",
    "    def inverse_single_column(self, series):\n",
    "        if self.column == None:\n",
    "            zeros_ = pd.DataFrame(np.zeros((series.shape[0], self.df.shape[1])))\n",
    "            zeros_[target_col] = series \n",
    "            return pd.DataFrame(self.scaler.inverse_transform(zeros_))[target_col]\n",
    "        else:\n",
    "            return pd.DataFrame(self.scaler.inverse_transform(series.reshape(-1, 1)))\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.split = [round(len(self.df) * 0.7), round(len(self.df) * 0.9)]\n",
    "\n",
    "        if self.inc != 1:\n",
    "            self.f_train, self.t_train = self.s_f[:self.split[0]], self.s_t[:self.split[0]]\n",
    "            self.f_valid, self.t_valid = self.s_f[self.split[0]:self.split[1]], self.s_t[self.split[0]:self.split[1]]\n",
    "            self.f_test, self.t_test = self.s_f[self.split[1]:], self.s_t[self.split[1]:]\n",
    "\n",
    "            self.plot_f_train, self.plot_t_train = self.windows[:self.split[0]], self.targets[:self.split[0]]\n",
    "            self.plot_f_valid, self.plot_t_valid = self.windows[self.split[0]:self.split[1]], self.targets[self.split[0]:self.split[1]]\n",
    "            self.plot_f_test, self.plot_t_test = self.windows[self.split[1]:], self.targets[self.split[1]:]\n",
    "        else:\n",
    "            self.f_train, self.t_train = self.windows[:self.split[0]], self.targets[:self.split[0]]\n",
    "            self.f_valid, self.t_valid = self.windows[self.split[0]:self.split[1]], self.targets[self.split[0]:self.split[1]]\n",
    "            self.f_test, self.t_test = self.windows[self.split[1]:], self.targets[self.split[1]:]\n",
    "        \n",
    "        print(f'Train: {self.f_train.shape}\\nValid: {self.f_valid.shape}\\nTest: {self.f_test.shape}')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(TensorDataset(self.f_train, self.t_train), batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(TensorDataset(self.f_train, self.t_train), batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(TensorDataset(self.f_train, self.t_train), batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    # def predict_dataloader(self):\n",
    "    #     return DataLoader(self.mnist_predict, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "def get_dates(weeks_ = 52):\n",
    "    today = datetime.now()\n",
    "\n",
    "    today_rounded_down = today.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    one_year_ago = today - timedelta(days=weeks_ * 7 + 1)\n",
    "\n",
    "    one_year_ago_rounded = one_year_ago.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    if one_year_ago.minute != 0 or one_year_ago.second != 0 or one_year_ago.microsecond != 0:\n",
    "        one_year_ago_rounded += timedelta(hours=1)\n",
    "\n",
    "    formatted_today_rounded_down = today_rounded_down.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    formatted_one_year_ago_rounded = one_year_ago_rounded.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    return formatted_one_year_ago_rounded, formatted_today_rounded_down, today\n",
    "\n",
    "def train(dm= None, folder='models_2', train_models=True, rfr=True, xgb_=True, knn=True, ridge=True, window_size=24, step=1):\n",
    "    if dm.column == None:\n",
    "        uni_multi = 'multi'\n",
    "    else:\n",
    "        uni_multi = 'uni'\n",
    "\n",
    "    if train_models:\n",
    "\n",
    "        if dm.inc != 1:\n",
    "            X_ = dm.f_train.reshape(-1, round(window_size / dm.inc))\n",
    "            X_valid = dm.f_valid.reshape(-1, round(window_size / dm.inc))\n",
    "        else:\n",
    "            X_ = dm.f_train.reshape(-1, window_size * dm.df.shape[1])\n",
    "            X_valid = dm.f_valid.reshape(-1, window_size * dm.df.shape[1])\n",
    "\n",
    "        if dm.column == None:\n",
    "            y_ = dm.t_train[:,:,target_col]\n",
    "            y_valid = dm.t_valid[:,:,target_col]\n",
    "        else:\n",
    "            y_ = dm.t_train\n",
    "            y_valid = dm.t_valid\n",
    "\n",
    "\n",
    "        if rfr:\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Training Random Forest Regressor...')\n",
    "\n",
    "            rf_regressor_1 = RandomForestRegressor(n_estimators=50, random_state=42)       \n",
    "        \n",
    "                \n",
    "            rf_regressor_1.fit(X_, y_)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_minutes = (end_time - start_time) / 60\n",
    "            print(f\"Elapsed minutes: {elapsed_minutes}\")\n",
    "            print('\\n\\n')\n",
    "\n",
    "            joblib.dump(rf_regressor_1, f'{folder}/random_forest_model_{uni_multi}_ws_{window_size}_s_{step}.pkl')\n",
    "                \n",
    "        if xgb_:\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Training XGBoost Model...')\n",
    "            \n",
    "            rf_regressor_xgb = xgb.XGBRegressor(base_score=0.5, booster='gbtree', learning_rate=0.01,\n",
    "                                                    max_depth=3, n_estimators=1000,\n",
    "                                                    objective='reg:linear', random_state=0)\n",
    "        \n",
    "        \n",
    "\n",
    "            rf_regressor_xgb.fit(X_, y_, eval_set=[(X_valid, y_valid)], \n",
    "                                early_stopping_rounds=10, \n",
    "                                verbose=False)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_minutes = (end_time - start_time) / 60\n",
    "            print(f\"Elapsed minutes: {elapsed_minutes}\")\n",
    "            print('\\n\\n')\n",
    "\n",
    "            model_path = f\"{folder}/xgboost_model_{uni_multi}_ws_{window_size}_s_{step}.bin\"\n",
    "            rf_regressor_xgb.save_model(model_path)  \n",
    "                \n",
    "        if knn:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print('Training KNN Regressor...')\n",
    "            knn_regressor = KNeighborsRegressor(n_neighbors=10) \n",
    "            knn_regressor.fit(X_, y_)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_minutes = (end_time - start_time) / 60\n",
    "            print(f\"Elapsed minutes: {elapsed_minutes}\")\n",
    "            print('\\n\\n')\n",
    "\n",
    "            joblib.dump(knn_regressor, f'{folder}/knn_regressor_model_{uni_multi}_ws_{window_size}_s_{step}.pkl')\n",
    "\n",
    "        if ridge:\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Training ridge Regressor...')\n",
    "            \n",
    "            ridge_model = Ridge(alpha=0.15)  \n",
    "\n",
    "            ridge_model.fit(X_, y_)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_minutes = (end_time - start_time) / 60 \n",
    "            print(f\"Elapsed minutes: {elapsed_minutes}\")    \n",
    "            print('\\n\\n')\n",
    "\n",
    "            joblib.dump(ridge_model, f'{folder}/ridge_regressor_model_{uni_multi}_ws_{window_size}_s_{step}.pkl')\n",
    "\n",
    "def load_models(dm = None, folder='models_2', window_size=24, step=1):\n",
    "    if dm.column == None:\n",
    "        uni_multi = 'multi'\n",
    "    else:\n",
    "        uni_multi = 'uni'\n",
    "\n",
    "    rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rfr_model = joblib.load(f'{folder}/random_forest_model_{uni_multi}_ws_{window_size}_s_{step}.pkl')\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "    xgb_model.load_model(f'{folder}/xgboost_model_{uni_multi}_ws_{window_size}_s_{step}.bin')\n",
    "    \n",
    "    knn_model = joblib.load(f'{folder}/knn_regressor_model_{uni_multi}_ws_{window_size}_s_{step}.pkl')\n",
    "\n",
    "    ridge_model = joblib.load(f'{folder}/ridge_regressor_model_{uni_multi}_ws_{window_size}_s_{step}.pkl')\n",
    "\n",
    "    print('Models loaded...')\n",
    "\n",
    "    return rfr_model, xgb_model, knn_model, ridge_model\n",
    "\n",
    "def plot_results(seed, height, width, interval, X, y, rfr_model, xgb_model, knn_model, ridge_model,\n",
    "                  plot_features=True, metrics=True, weights_ = [0.25, 0.25, 0.25, 0.25], window_size=24, step=1, dm=None):\n",
    "\n",
    "    mse_rfr = []\n",
    "    mse_xgb = []\n",
    "    mse_knn = []\n",
    "    mse_ridge = []\n",
    "    mse_avg = []\n",
    "\n",
    "    mse_tracker = {'Random Forest': mse_rfr, 'XGBoost': mse_xgb, 'kNN': mse_knn, 'Ridge': mse_ridge, 'Average': mse_avg}\n",
    "\n",
    "    fig, axes = plt.subplots(height,width, figsize=(18, 3 * height))\n",
    "\n",
    "    for i, ax_row in enumerate(axes):\n",
    "        for j, ax in enumerate(ax_row):\n",
    "            seed_index = i * 2 + j\n",
    "            seed = seed_index * interval \n",
    "            current_data = X[seed]\n",
    "\n",
    "            current_data_ = current_data.reshape(1, window_size * dm.df.shape[1])\n",
    "\n",
    "            step_pred_rfr = rfr_model.predict(current_data_).squeeze()\n",
    "            step_pred_xgb = xgb_model.predict(current_data_).squeeze()\n",
    "            step_pred_knn = knn_model.predict(current_data_).squeeze()\n",
    "            step_pred_ridge = ridge_model.predict(current_data_).squeeze()\n",
    "\n",
    "            average = step_pred_rfr * weights_[0] + step_pred_xgb * weights_[1] + step_pred_knn * weights_[2] + step_pred_ridge * weights_[3]\n",
    "\n",
    "            if dm.column == None:\n",
    "                t_test_data = y[seed:seed + step][0][:, target_col]\n",
    "                current_data = current_data[:, target_col]\n",
    "            else:\n",
    "                t_test_data = y[seed]\n",
    "\n",
    "            if plot_features:\n",
    "                ax.plot(range(window_size), current_data[:, 0], label='temperature', c='gray')\n",
    "                ax.plot(range(window_size), current_data[:, 1], label='humidity', c='gray')\n",
    "                ax.plot(range(window_size), current_data[:, 3], label='wind_direction', c='gray')\n",
    "                ax.plot(range(window_size), current_data[:, 4], label='wind_gusts', c='gray')\n",
    "\n",
    "            if dm.normalize_: \n",
    "                ax.plot(range(window_size), dm.inverse_single_column(current_data), label='wind_speed', c='black')\n",
    "                ax.plot(range(window_size, window_size + step), dm.inverse_single_column(t_test_data), label = 'Target', c='blue')\n",
    "                ax.plot(range(window_size, window_size + step), dm.inverse_single_column(step_pred_rfr), label = 'Reg', c='green')\n",
    "                ax.plot(range(window_size, window_size + step), dm.inverse_single_column(step_pred_xgb), label = 'XGB', c='red')\n",
    "                ax.plot(range(window_size, window_size + step), dm.inverse_single_column(step_pred_knn), label = 'kNN', c='violet')\n",
    "                ax.plot(range(window_size, window_size + step), dm.inverse_single_column(step_pred_ridge), label = 'Ridge', c='yellow')\n",
    "                ax.plot(range(window_size, window_size + step), dm.inverse_single_column(average), label = 'Average', c='orange', linewidth=2)\n",
    "\n",
    "                mse_rfr.append(mean_squared_error(dm.inverse_single_column(t_test_data), dm.inverse_single_column(step_pred_rfr)))\n",
    "                mse_xgb.append(mean_squared_error(dm.inverse_single_column(t_test_data), dm.inverse_single_column(step_pred_xgb)))\n",
    "                mse_knn.append(mean_squared_error(dm.inverse_single_column(t_test_data), dm.inverse_single_column(step_pred_knn)))\n",
    "                mse_ridge.append(mean_squared_error(dm.inverse_single_column(t_test_data), dm.inverse_single_column(step_pred_ridge)))\n",
    "                mse_avg.append(mean_squared_error(dm.inverse_single_column(t_test_data), dm.inverse_single_column(average)))\n",
    "\n",
    "            else:\n",
    "                ax.plot(range(window_size), current_data, label='wind_speed', c='black')\n",
    "                ax.plot(range(window_size, window_size + step), t_test_data, label = 'Target', c='blue')\n",
    "                ax.plot(range(window_size, window_size + step), step_pred_rfr, label = 'Reg', c='green')\n",
    "                ax.plot(range(window_size, window_size + step), step_pred_xgb, label = 'XGB', c='red')     \n",
    "                ax.plot(range(window_size, window_size + step), step_pred_knn, label = 'kNN', c='violet')\n",
    "                ax.plot(range(window_size, window_size + step), step_pred_ridge, label = 'Ridge', c='yellow')\n",
    "                ax.plot(range(window_size, window_size + step), average, label = 'Average', c='orange', linewidth=2)\n",
    "\n",
    "                mse_rfr.append(mean_squared_error(t_test_data, step_pred_rfr))\n",
    "                mse_xgb.append(mean_squared_error(t_test_data, step_pred_xgb))\n",
    "                mse_knn.append(mean_squared_error(t_test_data, step_pred_knn))\n",
    "                mse_ridge.append(mean_squared_error(t_test_data, step_pred_ridge))\n",
    "                mse_avg.append(mean_squared_error(t_test_data, average))\n",
    "\n",
    "            if i == 0 and j == 0:  \n",
    "                ax.legend(loc='upper left')\n",
    "\n",
    "            ax.set_title(f\"Seed: {seed}\")\n",
    "\n",
    "    if metrics:\n",
    "        for key, value in mse_tracker.items():\n",
    "            print(f'Mean MSE for {key}: {np.mean(value)}')\n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(mse_tracker.keys(), [np.mean(value) for value in mse_tracker.values()])\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Mean Squared Error (MSE)')\n",
    "    plt.title('MSE for Different Models')\n",
    "    plt.show()\n",
    "\n",
    "    return [np.mean(value) for value in mse_tracker.values()]\n",
    "\n",
    "def metrics(column_, X, y, rfr_model, xgb_model, knn_model, ridge_model, window_size, col_):    \n",
    "    weights = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "    if column_ == None:\n",
    "        y_true = y[:, :, target_col]\n",
    "    else:\n",
    "        y_true = y\n",
    "\n",
    "    X_valid = X.reshape(-1, window_size * col_)\n",
    "\n",
    "    y_pred_knn = knn_model.predict(X_valid)\n",
    "    y_pred_ridge = ridge_model.predict(X_valid)\n",
    "    y_pred_rfr = rfr_model.predict(X_valid)\n",
    "    y_pred_xgb = xgb_model.predict(X_valid)\n",
    "    y_pred_avg = y_pred_rfr * weights[0] + y_pred_xgb * weights[1] + y_pred_knn * weights[2] + y_pred_ridge * weights[3]\n",
    "\n",
    "\n",
    "    mse_avg = mean_squared_error(y_true, y_pred_avg)\n",
    "    mse_knn = mean_squared_error(y_true, y_pred_knn)\n",
    "    mse_ridge = mean_squared_error(y_true, y_pred_ridge)\n",
    "    mse_rfr = mean_squared_error(y_true, y_pred_rfr)\n",
    "    mse_xgb = mean_squared_error(y_true, y_pred_xgb)\n",
    "\n",
    "    # Print MSE for each model\n",
    "    print(\"MSE for Average model:\", mse_avg)\n",
    "    print(\"MSE for kNN model:\", mse_knn)\n",
    "    print(\"MSE for Ridge model:\", mse_ridge)\n",
    "    print(\"MSE for Random Forest model:\", mse_rfr)\n",
    "    print(\"MSE for XGBoost model:\", mse_xgb)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))  \n",
    "    plt.bar(['Average', 'kNN', 'Ridge', 'Random Forest', 'XGBoost'], [mse_avg, mse_knn, mse_ridge, mse_rfr, mse_xgb])\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Normalized Mean Squared Error (MSE)')\n",
    "    plt.title('MSE for Different Models')\n",
    "    plt.show()\n",
    "\n",
    "def build_model(hidden_size = [64, 32], out = 1, input_shape_ = 24 * 2, type_ = 'DNN'):\n",
    "    if type_ == 'DNN':\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_size[0], input_shape=(input_shape_,)))\n",
    "        model.add(Dense(hidden_size[1], activation='relu'))\n",
    "        model.add(Dense(out))\n",
    "    \n",
    "    elif type_ == 'LSTM':\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(hidden_size[0], input_shape=(input_shape_, 1)))\n",
    "        # model.add(Dense(hidden_size[1], activation='relu'))\n",
    "        model.add(Dense(out))\n",
    "    \n",
    "    elif type_ == 'GRU':\n",
    "        model = Sequential()\n",
    "        model.add(GRU(hidden_size[0], input_shape=(input_shape_, 1)))\n",
    "        # model.add(Dense(hidden_size[1], activation='relu'))\n",
    "        model.add(Dense(out))\n",
    "\n",
    "    elif type_ == 'CNN':\n",
    "        '''\n",
    "        The CNN is more effective when using less layers and filters\n",
    "\n",
    "        Hyperparameters will include:\n",
    "        - Number of filters or hidden size\n",
    "        - Kernel size\n",
    "        - Total layers\n",
    "        '''\n",
    "        model = tf.keras.Sequential([\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu', input_shape=(input_shape_, 1)),\n",
    "\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Dense(512, activation='relu'),\n",
    "\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(out, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    elif type_ == 'test':\n",
    "        model = tf.keras.Sequential([\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu', input_shape=(input_shape_, 1)),\n",
    "\n",
    "            Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            LSTM(64, activation='relu'),\n",
    "\n",
    "            Flatten(),            \n",
    "\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(out, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6132, 60)\n",
      "Valid: (1752, 60)\n",
      "Test: (484, 60)\n"
     ]
    }
   ],
   "source": [
    "window_size = 24*10\n",
    "step = 38\n",
    "inc = 4\n",
    "source = 'ERA'\n",
    "save_folder = 'stagger'\n",
    "train_reg = True\n",
    "train_deep = True\n",
    "\n",
    "dm = setup_data(data_=f'ERA5_Data\\ERA5_Reanalysis.csv', train_ = False, uni = True, window_size = window_size, step = step, sanity_check = False, tensor_=False, inc = inc)\n",
    "# dm = setup_data(data_=f'data/WindPowerAggregated_Eskom.csv', train_ = False, uni = True, window_size = window_size, step = step, sanity_check = False, tensor_=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, window_size, inc), dm\u001b[38;5;241m.\u001b[39mf_train[\u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# plt.plot(np.arange(0, window_size), dm.plot_f_train[0], label='Staggered Input', c='y')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(window_size, window_size\u001b[38;5;241m+\u001b[39mstep, inc),dm\u001b[38;5;241m.\u001b[39mt_train[\u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0, window_size, inc), dm.f_train[0], label='Input', c='black')\n",
    "# plt.plot(np.arange(0, window_size), dm.plot_f_train[0], label='Staggered Input', c='y')\n",
    "plt.plot(np.arange(window_size, window_size+step, inc),dm.t_train[0], label='Target', c='grey')\n",
    "# plt.plot(np.arange(window_size, window_size+step),dm.plot_t_train[0], label='Staggered Target', c='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Regressor...\n",
      "Elapsed minutes: 0.3645433187484741\n",
      "\n",
      "\n",
      "\n",
      "Training XGBoost Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [15:40:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed minutes: 0.0693741242090861\n",
      "\n",
      "\n",
      "\n",
      "Training KNN Regressor...\n",
      "Elapsed minutes: 6.674528121948242e-05\n",
      "\n",
      "\n",
      "\n",
      "Training ridge Regressor...\n",
      "Elapsed minutes: 0.0002481381098429362\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [15:40:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "if train_reg:\n",
    "    train(dm= dm, folder=save_folder, train_models=True, rfr=True, xgb_=True, knn=True, ridge=True, window_size=window_size, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def build_model(hidden_size = [64, 32], out = 1, input_shape_ = 24 * 2, type_ = 'DNN'):\n",
    "#     if type_ == 'DNN':\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(hidden_size[0], input_shape=(input_shape_,)))\n",
    "#         model.add(Dense(hidden_size[1], activation='relu'))\n",
    "#         model.add(Dense(out))\n",
    "    \n",
    "#     elif type_ == 'LSTM':\n",
    "#         model = Sequential()\n",
    "#         model.add(LSTM(hidden_size[0], input_shape=(input_shape_, 1)))\n",
    "#         # model.add(Dense(hidden_size[1], activation='relu'))\n",
    "#         model.add(Dense(out))\n",
    "    \n",
    "#     elif type_ == 'GRU':\n",
    "#         model = Sequential()\n",
    "#         model.add(GRU(hidden_size[0], input_shape=(input_shape_, 1)))\n",
    "#         # model.add(Dense(hidden_size[1], activation='relu'))\n",
    "#         model.add(Dense(out))\n",
    "\n",
    "#     elif type_ == 'CNN':\n",
    "#         '''\n",
    "#         The CNN is more effective when using less layers and filters\n",
    "\n",
    "#         Hyperparameters will include:\n",
    "#         - Number of filters or hidden size\n",
    "#         - Kernel size\n",
    "#         - Total layers\n",
    "#         '''\n",
    "#         model = tf.keras.Sequential([\n",
    "#             Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu', input_shape=(input_shape_, 1)),\n",
    "\n",
    "#             Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu'),\n",
    "\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "\n",
    "#             Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "#             Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "\n",
    "#             Flatten(),\n",
    "\n",
    "#             Dense(512, activation='relu'),\n",
    "\n",
    "#             Dropout(0.5),\n",
    "\n",
    "#             Dense(out, activation='sigmoid')\n",
    "#         ])\n",
    "\n",
    "#     elif type_ == 'test':\n",
    "#         model = tf.keras.Sequential([\n",
    "#             Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu', input_shape=(input_shape_, 1)),\n",
    "\n",
    "#             Conv1D(filters=hidden_size[0], kernel_size=3, activation='relu'),\n",
    "\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "\n",
    "#             Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "#             Conv1D(filters=hidden_size[1], kernel_size=3, activation='relu'),\n",
    "\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "\n",
    "#             LSTM(64, activation='relu'),\n",
    "\n",
    "#             Flatten(),            \n",
    "\n",
    "#             Dropout(0.5),\n",
    "\n",
    "#             Dense(out, activation='sigmoid')\n",
    "#         ])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN model\n",
      "Epoch 1/150\n",
      "192/192 [==============================] - 1s 2ms/step - loss: 0.0372 - val_loss: 0.0268\n",
      "Epoch 2/150\n",
      "103/192 [===============>..............] - ETA: 0s - loss: 0.0246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23603526\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0252\n",
      "Epoch 3/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0260\n",
      "Epoch 4/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0260\n",
      "Epoch 5/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 6/150\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0234 - val_loss: 0.0246\n",
      "Epoch 7/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0247\n",
      "Epoch 8/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0249\n",
      "Epoch 9/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0244\n",
      "Epoch 10/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0245\n",
      "Epoch 11/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0258\n",
      "Epoch 12/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 13/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0240\n",
      "Epoch 14/150\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0229 - val_loss: 0.0249\n",
      "Epoch 15/150\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 16/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0249\n",
      "Epoch 17/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0250\n",
      "Epoch 18/150\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0241\n",
      "Training LSTM model\n",
      "Epoch 1/150\n",
      "192/192 [==============================] - 4s 12ms/step - loss: 0.0343 - val_loss: 0.0283\n",
      "Epoch 2/150\n",
      "192/192 [==============================] - 2s 13ms/step - loss: 0.0253 - val_loss: 0.0270\n",
      "Epoch 3/150\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0245 - val_loss: 0.0261\n",
      "Epoch 4/150\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0242 - val_loss: 0.0264\n",
      "Epoch 5/150\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 0.0240 - val_loss: 0.0258\n",
      "Epoch 6/150\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0239 - val_loss: 0.0254\n",
      "Epoch 7/150\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0236 - val_loss: 0.0253\n",
      "Epoch 8/150\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.0235 - val_loss: 0.0244\n",
      "Epoch 9/150\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 0.0233 - val_loss: 0.0250\n",
      "Epoch 10/150\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0233 - val_loss: 0.0245\n",
      "Epoch 11/150\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0233 - val_loss: 0.0250\n",
      "Training GRU model\n",
      "Epoch 1/150\n",
      "192/192 [==============================] - 4s 12ms/step - loss: 0.0356 - val_loss: 0.0269\n",
      "Epoch 2/150\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 0.0243 - val_loss: 0.0270\n",
      "Epoch 3/150\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 0.0239 - val_loss: 0.0256\n",
      "Epoch 4/150\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.0238 - val_loss: 0.0257\n",
      "Epoch 5/150\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.0238 - val_loss: 0.0268\n",
      "Epoch 6/150\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 0.0237 - val_loss: 0.0252\n",
      "Epoch 7/150\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0236 - val_loss: 0.0255\n",
      "Epoch 8/150\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.0235 - val_loss: 0.0251\n",
      "Epoch 9/150\n",
      "192/192 [==============================] - 2s 13ms/step - loss: 0.0235 - val_loss: 0.0252\n",
      "Epoch 10/150\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.0235 - val_loss: 0.0252\n",
      "Epoch 11/150\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0234 - val_loss: 0.0248\n",
      "Epoch 12/150\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0234 - val_loss: 0.0246\n",
      "Epoch 13/150\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 14/150\n",
      "192/192 [==============================] - 2s 12ms/step - loss: 0.0232 - val_loss: 0.0241\n",
      "Epoch 15/150\n",
      "192/192 [==============================] - 2s 10ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 16/150\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.0230 - val_loss: 0.0242\n",
      "Epoch 17/150\n",
      "192/192 [==============================] - 2s 13ms/step - loss: 0.0230 - val_loss: 0.0242\n",
      "Training CNN model\n",
      "Epoch 1/150\n",
      "192/192 [==============================] - 3s 8ms/step - loss: 0.0319 - val_loss: 0.0282\n",
      "Epoch 2/150\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.0271\n",
      "Epoch 3/150\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0249 - val_loss: 0.0259\n",
      "Epoch 4/150\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0242 - val_loss: 0.0253\n",
      "Epoch 5/150\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0240 - val_loss: 0.0253\n",
      "Epoch 6/150\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0238 - val_loss: 0.0268\n",
      "Epoch 7/150\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 8/150\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 9/150\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0232 - val_loss: 0.0239\n",
      "Epoch 10/150\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0229 - val_loss: 0.0237\n",
      "Epoch 11/150\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0237\n",
      "Epoch 12/150\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0224 - val_loss: 0.0236\n",
      "Epoch 13/150\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0223 - val_loss: 0.0233\n",
      "Epoch 14/150\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0220 - val_loss: 0.0234\n",
      "Epoch 15/150\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 0.0218 - val_loss: 0.0238\n",
      "Epoch 16/150\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0216 - val_loss: 0.0243\n"
     ]
    }
   ],
   "source": [
    "if train_deep:\n",
    "    model_names = ['DNN', 'LSTM', 'GRU', 'CNN']\n",
    "\n",
    "    for name in model_names:\n",
    "        print(f'Training {name} model')\n",
    "        train_deep_models(dm, window_size, step, source, name, folder=save_folder, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
